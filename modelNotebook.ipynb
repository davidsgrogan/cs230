{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import scipy.io.wavfile\n",
    "from python_speech_features import mfcc, delta\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just read file number 1 which contains 7305509 audio samples and is named cousinhenry_01_trollope_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n",
      "\t analyzing training sample number 1000\n",
      "\t analyzing training sample number 1500\n",
      "just read file number 2 which contains 12400013 audio samples and is named siegeofcorinth_2_byron_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n",
      "\t analyzing training sample number 1000\n",
      "\t analyzing training sample number 1500\n",
      "\t analyzing training sample number 2000\n",
      "\t analyzing training sample number 2500\n",
      "\t analyzing training sample number 3000\n",
      "just read file number 3 which contains 36554719 audio samples and is named upperroom_16_ryle_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n",
      "\t analyzing training sample number 1000\n",
      "\t analyzing training sample number 1500\n",
      "\t analyzing training sample number 2000\n",
      "\t analyzing training sample number 2500\n",
      "\t analyzing training sample number 3000\n",
      "\t analyzing training sample number 3500\n",
      "\t analyzing training sample number 4000\n",
      "\t analyzing training sample number 4500\n",
      "\t analyzing training sample number 5000\n",
      "\t analyzing training sample number 5500\n",
      "\t analyzing training sample number 6000\n",
      "\t analyzing training sample number 6500\n",
      "\t analyzing training sample number 7000\n",
      "\t analyzing training sample number 7500\n",
      "\t analyzing training sample number 8000\n",
      "\t analyzing training sample number 8500\n",
      "\t analyzing training sample number 9000\n",
      "just read file number 4 which contains 3008270 audio samples and is named vorst_14_machiavelli_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "#files = [ 'vorst_14_machiavelli_8khz.wav'\n",
    "# ]\n",
    "files = [ 'cousinhenry_01_trollope_8khz.wav',\n",
    "'siegeofcorinth_2_byron_8khz.wav',\n",
    "'upperroom_16_ryle_8khz.wav',\n",
    "'vorst_14_machiavelli_8khz.wav',\n",
    "]\n",
    "\n",
    "#more comments about this calculation? +1 - bias?\n",
    "height_of_one_training_example = 49 * 13 * 2 + 1\n",
    "\n",
    "label = 0\n",
    "all_examples = []\n",
    "for one_file in files:\n",
    "  label += 1\n",
    "  rate, data = scipy.io.wavfile.read(one_file)\n",
    "  total_length_of_wave = data.shape[0]\n",
    "  print (\"just read file number %d which contains %d audio samples and is named %s Now analying it:\" % (label, total_length_of_wave, one_file))\n",
    "  assert rate == 8000, \"rate was %d\" % rate\n",
    "\n",
    "  half_second_length = 4000\n",
    "  start_index_of_half_second = 0\n",
    "  num_training_example_in_this_file = 0\n",
    "  while total_length_of_wave - start_index_of_half_second >= half_second_length:\n",
    "    num_training_example_in_this_file += 1\n",
    "    if num_training_example_in_this_file % 500 == 0:\n",
    "      print (\"\\t analyzing training sample number %d\" % num_training_example_in_this_file)\n",
    "\n",
    "    this_training_example_raw = data[start_index_of_half_second:start_index_of_half_second + half_second_length]\n",
    "    start_index_of_half_second += half_second_length\n",
    "    assert len(this_training_example_raw) == 4000, len(this_training_example_raw)\n",
    "    mfccs = mfcc(this_training_example_raw, 8000)\n",
    "    assert mfccs.shape == (49, 13), mfccs.shape\n",
    "\n",
    "    #Alfredo used 2 here, and changing it doesn't change the output size.\n",
    "    first_derivative = delta(mfccs, 2)\n",
    "    assert first_derivative.shape == (49, 13), first_derivative.shape\n",
    "    all_examples.extend(mfccs.flatten().tolist())\n",
    "    all_examples.extend(first_derivative.flatten().tolist())\n",
    "    all_examples.append(label)\n",
    "    assert len(all_examples) % height_of_one_training_example == 0, \"num_training_example_in_this_file = %d\" % num_training_example_in_this_file\n",
    "\n",
    "all_examples_np = np.array(all_examples)\n",
    "all_examples_np = all_examples_np.reshape((height_of_one_training_example, -1), order='F')\n",
    "\n",
    "#print (\"all_examples_np.shape = %s, so we have %d training samples\" % (all_examples_np.shape, all_examples_np.shape[1]))\n",
    "assert all_examples_np[-1, 0] == 1, \"make sure the last row labels the first column as belonging to file number 1 %s\" % all_examples_np[-1, 0]\n",
    "\n",
    "shuffled_examples = all_examples_np.T\n",
    "np.random.shuffle(shuffled_examples)\n",
    "shuffled_examples = shuffled_examples.T\n",
    "\n",
    "training_pct = 0.8\n",
    "\n",
    "number_of_training_examples = int(math.ceil(all_examples_np.shape[1] * training_pct))\n",
    "\n",
    "X_train = shuffled_examples[0:-1, 0:number_of_training_examples]\n",
    "Y_train = shuffled_examples[-1:, 0:number_of_training_examples]\n",
    "X_dev   = shuffled_examples[0:-1, number_of_training_examples:]\n",
    "Y_dev   = shuffled_examples[-1:, number_of_training_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11853, 1274) (11853, 1) (2963, 1274) (2963, 1)\n",
      "[[3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]]\n"
     ]
    }
   ],
   "source": [
    "# Xs are shape (number of input features, number of data points)\n",
    "# Ys are shape (1, number of data points)\n",
    "# The labels in Y are an integer corresponding to the speaker number.\n",
    "# Before reshape\n",
    "# (1274, 11853) (1, 11853) (1274, 2963) (1, 2963)\n",
    "\n",
    "# In Keras, you want (number of data, attributes)\n",
    "# Want: (see coursera M4 - Keras Tutorial)\n",
    "# (11853, 1274) (11853, 1) (2963, 1274) (2963, 1)\n",
    "# Reshape\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.T\n",
    "X_dev = X_dev.T\n",
    "Y_dev = Y_dev.T\n",
    "\n",
    "#check\n",
    "print(X_train.shape, Y_train.shape, X_dev.shape, Y_dev.shape)\n",
    "\n",
    "#debugging - to be deleted\n",
    "print(Y_train[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11853 samples, validate on 2963 samples\n",
      "Epoch 1/10\n",
      "11853/11853 [==============================] - 0s 37us/sample - loss: 5.7648 - acc: 0.0129 - val_loss: 3.1760 - val_acc: 0.0019\n",
      "Epoch 2/10\n",
      "11853/11853 [==============================] - 0s 19us/sample - loss: 3.1645 - acc: 7.5930e-04 - val_loss: 3.1739 - val_acc: 0.0014\n",
      "Epoch 3/10\n",
      "11853/11853 [==============================] - 0s 19us/sample - loss: 3.1540 - acc: 4.0074e-04 - val_loss: 3.1634 - val_acc: 0.0013\n",
      "Epoch 4/10\n",
      "11853/11853 [==============================] - 0s 19us/sample - loss: 3.1569 - acc: 2.3201e-04 - val_loss: 3.1621 - val_acc: 9.2811e-04\n",
      "Epoch 5/10\n",
      "11853/11853 [==============================] - 0s 18us/sample - loss: 3.1500 - acc: 6.3275e-05 - val_loss: 3.1585 - val_acc: 0.0010\n",
      "Epoch 6/10\n",
      "11853/11853 [==============================] - 0s 18us/sample - loss: 3.1531 - acc: 6.3275e-05 - val_loss: 3.1585 - val_acc: 7.5937e-04\n",
      "Epoch 7/10\n",
      "11853/11853 [==============================] - 0s 19us/sample - loss: 3.1483 - acc: 6.3275e-05 - val_loss: 3.1560 - val_acc: 9.2811e-04\n",
      "Epoch 8/10\n",
      "11853/11853 [==============================] - 0s 19us/sample - loss: 3.1483 - acc: 2.1092e-05 - val_loss: 3.1557 - val_acc: 8.4374e-04\n",
      "Epoch 9/10\n",
      "11853/11853 [==============================] - 0s 19us/sample - loss: 3.1456 - acc: 4.2183e-05 - val_loss: 3.1584 - val_acc: 9.2811e-04\n",
      "Epoch 10/10\n",
      "11853/11853 [==============================] - 0s 23us/sample - loss: 3.1479 - acc: 1.0546e-04 - val_loss: 3.1565 - val_acc: 7.5937e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a31c74a20>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Keras\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#hidden layers - 50 nodes\n",
    "#possible activation functions for hidden layers in Keras: elu (Exponential linear unit), selu (Scaled Exponential Linear Unit), \n",
    "#tanh, sigmoid, exponential, linear\n",
    "#https://keras.io/activations/\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "# Add another (optional):\n",
    "#model.add(layers.Dense(50, activation='relu'))\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "#justification for binary: \n",
    "#\"we compile the model using binary cross-entropy rather than categorical cross-entropy. This may seem counterintuitive \n",
    "# for multi-label classification; however, the goal is to treat each output label as an independent Bernoulli distribution \n",
    "# and we want to penalize each output node independently.\"\n",
    "#quoted from: https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/\n",
    "#more: https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance/46038271\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=150, verbose=1, shuffle=True,\n",
    "         validation_data=(X_dev, Y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
