{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import scipy.io.wavfile\n",
    "from python_speech_features import mfcc, delta\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just read file number 1 which contains 7305509 audio samples and is named cousinhenry_01_trollope_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n",
      "\t analyzing training sample number 1000\n",
      "\t analyzing training sample number 1500\n",
      "just read file number 2 which contains 12400013 audio samples and is named siegeofcorinth_2_byron_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n",
      "\t analyzing training sample number 1000\n",
      "\t analyzing training sample number 1500\n",
      "\t analyzing training sample number 2000\n",
      "\t analyzing training sample number 2500\n",
      "\t analyzing training sample number 3000\n",
      "just read file number 3 which contains 36554719 audio samples and is named upperroom_16_ryle_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n",
      "\t analyzing training sample number 1000\n",
      "\t analyzing training sample number 1500\n",
      "\t analyzing training sample number 2000\n",
      "\t analyzing training sample number 2500\n",
      "\t analyzing training sample number 3000\n",
      "\t analyzing training sample number 3500\n",
      "\t analyzing training sample number 4000\n",
      "\t analyzing training sample number 4500\n",
      "\t analyzing training sample number 5000\n",
      "\t analyzing training sample number 5500\n",
      "\t analyzing training sample number 6000\n",
      "\t analyzing training sample number 6500\n",
      "\t analyzing training sample number 7000\n",
      "\t analyzing training sample number 7500\n",
      "\t analyzing training sample number 8000\n",
      "\t analyzing training sample number 8500\n",
      "\t analyzing training sample number 9000\n",
      "just read file number 4 which contains 3008270 audio samples and is named vorst_14_machiavelli_8khz.wav Now analying it:\n",
      "\t analyzing training sample number 500\n",
      "CPU times: user 27.5 s, sys: 19.9 s, total: 47.4 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "np.random.seed(1234)\n",
    "\n",
    "#files = [ 'vorst_14_machiavelli_8khz.wav'\n",
    "# ]\n",
    "files = [ 'cousinhenry_01_trollope_8khz.wav',\n",
    "'siegeofcorinth_2_byron_8khz.wav',\n",
    "'upperroom_16_ryle_8khz.wav',\n",
    "'vorst_14_machiavelli_8khz.wav',\n",
    "]\n",
    "\n",
    "# This constant is the number of entries in the all_examples list below that\n",
    "# correspond to one training example.\n",
    "# The 13 is magic -- it's the default number of mfcc... frequencies(?) calculated\n",
    "# by many places on the web and in the python library we used.\n",
    "# The 49 comes from a combination of our sample size and mfcc window. Our sample\n",
    "# size is half second and we calculate mfccs in a 25ms window with a 10ms stride.\n",
    "# (.5 - .01) / 0.01\n",
    "\n",
    "in 1/2 second of an 8000hz wave, we get 49 values for each of the 13 frequencies.\n",
    "# Multiply that * 2 because we are also calculating the first derivative of the\n",
    "# mfcc... cepstrum(?), which seems to be common (further adding 2nd derivative\n",
    "# is also common).\n",
    "# The + 1 is because the label for each sample comes right after the above.\n",
    "height_of_one_training_example = 49 * 13 * 2 + 1\n",
    "\n",
    "label = 0\n",
    "all_examples = []\n",
    "for one_file in files:\n",
    "  label += 1\n",
    "  rate, data = scipy.io.wavfile.read(one_file)\n",
    "  total_length_of_wave = data.shape[0]\n",
    "  print (\"just read file number %d which contains %d audio samples and is named %s Now analying it:\" % (label, total_length_of_wave, one_file))\n",
    "  assert rate == 8000, \"rate was %d\" % rate\n",
    "\n",
    "  half_second_length = 4000\n",
    "  start_index_of_half_second = 0\n",
    "  num_training_example_in_this_file = 0\n",
    "  while total_length_of_wave - start_index_of_half_second >= half_second_length:\n",
    "    num_training_example_in_this_file += 1\n",
    "    if num_training_example_in_this_file % 500 == 0:\n",
    "      print (\"\\t analyzing training sample number %d\" % num_training_example_in_this_file)\n",
    "\n",
    "    this_training_example_raw = data[start_index_of_half_second:start_index_of_half_second + half_second_length]\n",
    "    start_index_of_half_second += half_second_length\n",
    "    assert len(this_training_example_raw) == 4000, len(this_training_example_raw)\n",
    "    mfccs = mfcc(this_training_example_raw, 8000)\n",
    "    assert mfccs.shape == (49, 13), mfccs.shape\n",
    "\n",
    "    #Alfredo used 2 here, and changing it doesn't change the output size.\n",
    "    first_derivative = delta(mfccs, 2)\n",
    "    assert first_derivative.shape == (49, 13), first_derivative.shape\n",
    "    all_examples.extend(mfccs.flatten().tolist())\n",
    "    all_examples.extend(first_derivative.flatten().tolist())\n",
    "    all_examples.append(label)\n",
    "    assert len(all_examples) % height_of_one_training_example == 0, \"num_training_example_in_this_file = %d\" % num_training_example_in_this_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13335, 1274) (13335, 1) (1481, 1274) (1481, 1)\n",
      "[[3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "CPU times: user 1.12 s, sys: 162 ms, total: 1.28 s\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(123456)\n",
    "\n",
    "# Make a dataset out of the single python list constructed above.\n",
    "# TODO(dgrogan): Parts of the first half and last half of this cell are inverses and can be combined.\n",
    "all_examples_np = np.array(all_examples)\n",
    "all_examples_np = all_examples_np.reshape((height_of_one_training_example, -1), order='F')\n",
    "\n",
    "#print (\"all_examples_np.shape = %s, so we have %d training samples\" % (all_examples_np.shape, all_examples_np.shape[1]))\n",
    "assert all_examples_np[-1, 0] == 1, \"make sure the last row labels the first column as belonging to file number 1 %s\" % all_examples_np[-1, 0]\n",
    "\n",
    "shuffled_examples = all_examples_np.T\n",
    "np.random.shuffle(shuffled_examples)\n",
    "shuffled_examples = shuffled_examples.T\n",
    "\n",
    "# I changed to 0.9 when I thought we might not have enough data. We can change it back to whatever.\n",
    "training_pct = 0.9\n",
    "\n",
    "number_of_training_examples = int(math.ceil(all_examples_np.shape[1] * training_pct))\n",
    "\n",
    "X_train = shuffled_examples[0:-1, 0:number_of_training_examples]\n",
    "Y_train = shuffled_examples[-1:, 0:number_of_training_examples]\n",
    "X_dev   = shuffled_examples[0:-1, number_of_training_examples:]\n",
    "Y_dev   = shuffled_examples[-1:, number_of_training_examples:]\n",
    "\n",
    "# Xs are shape (number of input features, number of data points)\n",
    "# Ys are shape (1, number of data points)\n",
    "# The labels in Y are an integer corresponding to the speaker number.\n",
    "# Before reshape\n",
    "# (1274, 11853) (1, 11853) (1274, 2963) (1, 2963)\n",
    "\n",
    "# In Keras, you want (number of data, attributes)\n",
    "# Want: (see coursera M4 - Keras Tutorial)\n",
    "# (11853, 1274) (11853, 1) (2963, 1274) (2963, 1)\n",
    "# Reshape\n",
    "X_train = X_train.T\n",
    "Y_train = Y_train.T\n",
    "X_dev = X_dev.T\n",
    "Y_dev = Y_dev.T\n",
    "\n",
    "#check\n",
    "print(X_train.shape, Y_train.shape, X_dev.shape, Y_dev.shape)\n",
    "\n",
    "#debugging - to be deleted\n",
    "print(Y_train[110:120])\n",
    "Y_train = to_categorical(Y_train - 1) # -1 because to_categorical seems to expect labels to start at 0\n",
    "Y_dev = to_categorical(Y_dev - 1)\n",
    "print(Y_train[110:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Keras\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "#hidden layers\n",
    "model.add(layers.Dense(5, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "#justification for binary: \n",
    "#\"we compile the model using binary cross-entropy rather than categorical cross-entropy. This may seem counterintuitive \n",
    "# for multi-label classification; however, the goal is to treat each output label as an independent Bernoulli distribution \n",
    "# and we want to penalize each output node independently.\"\n",
    "#quoted from: https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/\n",
    "#more: https://stackoverflow.com/questions/42081257/keras-binary-crossentropy-vs-categorical-crossentropy-performance/46038271\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "import keras.utils\n",
    "import IPython.display\n",
    "keras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True)\n",
    "display(IPython.display.Image('test_keras_plot_model.png'))\n",
    "print(model.summary())\n",
    "# I don't know where 140240419526080 in the picture came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13335 samples, validate on 1481 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 1.1132 - acc: 0.6841 - val_loss: 0.6287 - val_acc: 0.7711\n",
      "Epoch 2/50\n",
      " - 1s - loss: 0.5521 - acc: 0.8040 - val_loss: 0.5275 - val_acc: 0.8103\n",
      "Epoch 3/50\n",
      " - 1s - loss: 0.4887 - acc: 0.8164 - val_loss: 0.4902 - val_acc: 0.8116\n",
      "Epoch 4/50\n",
      " - 1s - loss: 0.4239 - acc: 0.8352 - val_loss: 0.3937 - val_acc: 0.8798\n",
      "Epoch 5/50\n",
      " - 1s - loss: 0.3391 - acc: 0.8886 - val_loss: 0.3515 - val_acc: 0.9169\n",
      "Epoch 6/50\n",
      " - 1s - loss: 0.3035 - acc: 0.9207 - val_loss: 0.3407 - val_acc: 0.9244\n",
      "Epoch 7/50\n",
      " - 1s - loss: 0.2777 - acc: 0.9282 - val_loss: 0.3363 - val_acc: 0.9291\n",
      "Epoch 8/50\n",
      " - 1s - loss: 0.2606 - acc: 0.9330 - val_loss: 0.3074 - val_acc: 0.9338\n",
      "Epoch 9/50\n",
      " - 1s - loss: 0.2422 - acc: 0.9393 - val_loss: 0.3105 - val_acc: 0.9332\n",
      "Epoch 10/50\n",
      " - 1s - loss: 0.2277 - acc: 0.9429 - val_loss: 0.3065 - val_acc: 0.9426\n",
      "Epoch 11/50\n",
      " - 1s - loss: 0.2127 - acc: 0.9463 - val_loss: 0.2942 - val_acc: 0.9446\n",
      "Epoch 12/50\n",
      " - 1s - loss: 0.2025 - acc: 0.9482 - val_loss: 0.2894 - val_acc: 0.9406\n",
      "Epoch 13/50\n",
      " - 1s - loss: 0.1933 - acc: 0.9507 - val_loss: 0.2819 - val_acc: 0.9406\n",
      "Epoch 14/50\n",
      " - 1s - loss: 0.1825 - acc: 0.9516 - val_loss: 0.2908 - val_acc: 0.9365\n",
      "Epoch 15/50\n",
      " - 1s - loss: 0.1741 - acc: 0.9540 - val_loss: 0.2870 - val_acc: 0.9352\n",
      "Epoch 16/50\n",
      " - 1s - loss: 0.1692 - acc: 0.9558 - val_loss: 0.2807 - val_acc: 0.9379\n",
      "Epoch 17/50\n",
      " - 1s - loss: 0.1640 - acc: 0.9552 - val_loss: 0.2878 - val_acc: 0.9325\n",
      "Epoch 18/50\n",
      " - 1s - loss: 0.1645 - acc: 0.9531 - val_loss: 0.2859 - val_acc: 0.9392\n",
      "Epoch 19/50\n",
      " - 1s - loss: 0.1557 - acc: 0.9566 - val_loss: 0.2745 - val_acc: 0.9372\n",
      "Epoch 20/50\n",
      " - 1s - loss: 0.1492 - acc: 0.9585 - val_loss: 0.2785 - val_acc: 0.9413\n",
      "Epoch 21/50\n",
      " - 1s - loss: 0.1490 - acc: 0.9578 - val_loss: 0.2682 - val_acc: 0.9413\n",
      "Epoch 22/50\n",
      " - 1s - loss: 0.1412 - acc: 0.9611 - val_loss: 0.2775 - val_acc: 0.9392\n",
      "Epoch 23/50\n",
      " - 1s - loss: 0.1383 - acc: 0.9593 - val_loss: 0.2770 - val_acc: 0.9453\n",
      "Epoch 24/50\n",
      " - 1s - loss: 0.1342 - acc: 0.9618 - val_loss: 0.2770 - val_acc: 0.9440\n",
      "Epoch 25/50\n",
      " - 1s - loss: 0.1309 - acc: 0.9636 - val_loss: 0.2657 - val_acc: 0.9406\n",
      "Epoch 26/50\n",
      " - 1s - loss: 0.1267 - acc: 0.9637 - val_loss: 0.2658 - val_acc: 0.9359\n",
      "Epoch 27/50\n",
      " - 1s - loss: 0.1241 - acc: 0.9650 - val_loss: 0.2795 - val_acc: 0.9460\n",
      "Epoch 28/50\n",
      " - 1s - loss: 0.1235 - acc: 0.9642 - val_loss: 0.2691 - val_acc: 0.9413\n",
      "Epoch 29/50\n",
      " - 1s - loss: 0.1220 - acc: 0.9638 - val_loss: 0.2628 - val_acc: 0.9460\n",
      "Epoch 30/50\n",
      " - 1s - loss: 0.1217 - acc: 0.9634 - val_loss: 0.2601 - val_acc: 0.9440\n",
      "Epoch 31/50\n",
      " - 1s - loss: 0.1184 - acc: 0.9642 - val_loss: 0.2718 - val_acc: 0.9311\n",
      "Epoch 32/50\n",
      " - 1s - loss: 0.1169 - acc: 0.9651 - val_loss: 0.2565 - val_acc: 0.9446\n",
      "Epoch 33/50\n",
      " - 1s - loss: 0.1138 - acc: 0.9663 - val_loss: 0.2519 - val_acc: 0.9345\n",
      "Epoch 34/50\n",
      " - 1s - loss: 0.1103 - acc: 0.9667 - val_loss: 0.2559 - val_acc: 0.9440\n",
      "Epoch 35/50\n",
      " - 1s - loss: 0.1067 - acc: 0.9675 - val_loss: 0.2544 - val_acc: 0.9419\n",
      "Epoch 36/50\n",
      " - 1s - loss: 0.1064 - acc: 0.9678 - val_loss: 0.2777 - val_acc: 0.9419\n",
      "Epoch 37/50\n",
      " - 1s - loss: 0.1041 - acc: 0.9677 - val_loss: 0.2697 - val_acc: 0.9365\n",
      "Epoch 38/50\n",
      " - 1s - loss: 0.1057 - acc: 0.9669 - val_loss: 0.2718 - val_acc: 0.9365\n",
      "Epoch 39/50\n",
      " - 1s - loss: 0.1050 - acc: 0.9671 - val_loss: 0.2717 - val_acc: 0.9419\n",
      "Epoch 40/50\n",
      " - 1s - loss: 0.1003 - acc: 0.9684 - val_loss: 0.2700 - val_acc: 0.9413\n",
      "Epoch 41/50\n",
      " - 1s - loss: 0.1019 - acc: 0.9669 - val_loss: 0.2720 - val_acc: 0.9399\n",
      "Epoch 42/50\n",
      " - 1s - loss: 0.1023 - acc: 0.9663 - val_loss: 0.2834 - val_acc: 0.9392\n",
      "Epoch 43/50\n",
      " - 1s - loss: 0.0996 - acc: 0.9678 - val_loss: 0.2826 - val_acc: 0.9419\n",
      "Epoch 44/50\n",
      " - 1s - loss: 0.0946 - acc: 0.9687 - val_loss: 0.2747 - val_acc: 0.9433\n",
      "Epoch 45/50\n",
      " - 1s - loss: 0.0861 - acc: 0.9709 - val_loss: 0.2849 - val_acc: 0.9440\n",
      "Epoch 46/50\n",
      " - 1s - loss: 0.0838 - acc: 0.9735 - val_loss: 0.2904 - val_acc: 0.9433\n",
      "Epoch 47/50\n",
      " - 1s - loss: 0.0843 - acc: 0.9725 - val_loss: 0.2721 - val_acc: 0.9399\n",
      "Epoch 48/50\n",
      " - 1s - loss: 0.0872 - acc: 0.9713 - val_loss: 0.2793 - val_acc: 0.9480\n",
      "Epoch 49/50\n",
      " - 1s - loss: 0.0813 - acc: 0.9730 - val_loss: 0.2848 - val_acc: 0.9480\n",
      "Epoch 50/50\n",
      " - 1s - loss: 0.0807 - acc: 0.9737 - val_loss: 0.2939 - val_acc: 0.9453\n",
      "CPU times: user 48.9 s, sys: 10.1 s, total: 58.9 s\n",
      "Wall time: 30.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history_object = model.fit(X_train, Y_train, epochs=50, batch_size=128, verbose=2, shuffle=True,\n",
    "         validation_data=(X_dev, Y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.628659961716216, 0.5274683567094771, 0.49015536910371954, 0.3936888441332766, 0.35152969190914996, 0.34065694699490257, 0.3363025250425216, 0.30740239656832796, 0.3104892758643764, 0.3065342222573707, 0.2942391063296095, 0.2894254554613952, 0.28193381579641874, 0.29079900788624974, 0.2870364790627636, 0.28071054418814656, 0.28783402961705845, 0.2858752277475366, 0.2745104163462531, 0.27848368815720526, 0.26819568455903453, 0.2775131061240518, 0.27702945935150647, 0.2770496806548625, 0.2657497778766305, 0.26580034327177, 0.279494552058718, 0.2690821038074867, 0.2627817449398253, 0.2601203169575581, 0.27183974186646465, 0.2565397036530374, 0.2519127376412798, 0.2558683898000601, 0.2544284842802736, 0.2776728342567079, 0.26972453046658007, 0.2718190604230989, 0.27168635497275917, 0.2700019520251514, 0.27203720121831204, 0.28335269749325565, 0.282552713793649, 0.27472177219805405, 0.28494654322177637, 0.2904336956064899, 0.2721321642358588, 0.2793166618303702, 0.2848029532630165, 0.2939180506847341], 'val_acc': [0.7711006062888841, 0.8102633345376632, 0.8116137734303034, 0.8798109378708447, 0.9169480088673114, 0.924375423139048, 0.9291019566875306, 0.9338284950253128, 0.933153273365451, 0.9426063481896895, 0.9446320039528918, 0.9405806872749715, 0.9405806872749715, 0.9365293709592671, 0.9351789320666271, 0.9378798098519072, 0.9324780542813468, 0.9392302483823314, 0.9372045900433712, 0.9412559067212916, 0.9412559070835075, 0.9392302487445473, 0.9453072237614278, 0.9439567848687878, 0.9405806876371874, 0.9358541511507311, 0.9459824428455319, 0.9412559070835075, 0.9459824432077478, 0.9439567848687878, 0.9311276153887067, 0.9446320039528918, 0.934503712258091, 0.9439567848687878, 0.9419311261676117, 0.9419311265298276, 0.9365293709592671, 0.9365293709592671, 0.9419311265298276, 0.9412559070835075, 0.9399054681908674, 0.9392302487445473, 0.9419311265298276, 0.9432815650602517, 0.9439567845065717, 0.9432815650602517, 0.9399054681908674, 0.948008101546708, 0.9480081011844921, 0.9453072237614278], 'loss': [1.1131659641442873, 0.5521186785435114, 0.48871147641314966, 0.42385888489212575, 0.33911592367775306, 0.30346634551284907, 0.2776900677453904, 0.26056670017308586, 0.2422182296539989, 0.22771192420096625, 0.21266883419980975, 0.20251085751281053, 0.19328687102507686, 0.18249157794660784, 0.17405919728651895, 0.16924817621383856, 0.1640121861191604, 0.16448640892757743, 0.15572233415852338, 0.14919017362245962, 0.1490228740673723, 0.14123367899627243, 0.13832381421376758, 0.1341913110963137, 0.1309126826621282, 0.1267180209732878, 0.12412874221265383, 0.12352500526275713, 0.1220149539637351, 0.12170677890782505, 0.11838815522034114, 0.11687618871004697, 0.11378266255552509, 0.11029315955244091, 0.10670272203551324, 0.10643333548161034, 0.10414216313462245, 0.10568715783264201, 0.1050346646900699, 0.1002963481017678, 0.10193627770888167, 0.1023259804373517, 0.09958078121737814, 0.09458213057976605, 0.08605368973706148, 0.08379333226990066, 0.08426434445747687, 0.0872279820897392, 0.08131229712704303, 0.0806518654509077], 'acc': [0.6841394825333909, 0.8039745031736922, 0.8164229471673669, 0.835245594287303, 0.8886389201349831, 0.9206599175103112, 0.9281589801632424, 0.9330333708286465, 0.9392575928008999, 0.9428571428929011, 0.9463067116789216, 0.9481814773153355, 0.9506561679790027, 0.9515560555109426, 0.9540307461746096, 0.9558305211848519, 0.9552305962112364, 0.9531308586426697, 0.9565804274644484, 0.9584551931366208, 0.9577802774831959, 0.9610798650168729, 0.9592800899887514, 0.9617547806032508, 0.9635545556805399, 0.9637045369507625, 0.964979377528635, 0.9642294713160855, 0.9637795275769343, 0.9634045744460756, 0.9642294713160855, 0.9651293588480254, 0.9662542181735545, 0.9667041619797525, 0.9675290588855207, 0.9678290213723285, 0.967679040119985, 0.966929133894026, 0.9670791151284903, 0.9684289463817023, 0.9669291338582677, 0.9662542182227222, 0.9678290213723285, 0.9686539182602175, 0.9709036370453693, 0.973528308979259, 0.9724784402128548, 0.971278590176228, 0.9730033745960569, 0.9736782902316025]}\n"
     ]
    }
   ],
   "source": [
    "print(history_object.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230 venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
